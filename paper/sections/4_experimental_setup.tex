\section{Experimental Setup}

\subsection{Datasets}

\textbf{Speech Data}: We use LibriTTS~\citep{libritts}, a multi-speaker English corpus derived from audiobooks. For controlled evaluation, we focus on speaker 5339, using 168 utterances totaling approximately 12 minutes of speech. We split the data into 100 training samples and 40 test samples for training attack evaluation, with additional subsets for zero-shot and robustness experiments.

\textbf{Noise Data}: We use the TAU Urban Acoustic Scenes 2022 Mobile Development dataset~\citep{tau_dataset}, which contains authentic recordings from 10 urban scenes captured with mobile devices. The dataset provides diverse acoustic contexts including transportation hubs (airport, bus, metro), public spaces (park, square, mall), and street environments. We extract 50,000 three-second clips (5,000 per scene) for our noise library.

\subsection{Models}

\textbf{Speaker Verification}: We use ECAPA-TDNN~\citep{ecapa}, a state-of-the-art speaker verification model from the SpeechBrain toolkit. The model extracts 192-dimensional speaker embeddings, which we use to compute speaker similarity via cosine distance. ECAPA-TDNN achieves strong performance on VoxCeleb and is widely used for speaker verification tasks.

\textbf{Automatic Speech Recognition}: We employ Whisper Base~\citep{whisper}, an encoder-decoder transformer trained on 680,000 hours of multilingual speech data. Whisper provides robust transcription for measuring speech intelligibility via word error rate (WER). We use the base model (74M parameters) for computational efficiency.

\textbf{TTS Baseline}: For training attack experiments, we reference BERT-VITS2~\citep{bertvits2}, a recent TTS architecture that combines BERT-based text encoding with VITS neural vocoder. While we do not perform full TTS training due to computational constraints, we use speaker embedding degradation as a proxy for TTS quality, following established evaluation protocols~\citep{safespeech}.

\subsection{Optimization Hyperparameters}

SceneGuard employs gradient-based optimization to jointly learn the temporal mask $m(t)$ and noise strength $\gamma$. We describe the key hyperparameters and design choices:

\textbf{Optimization Objective}: We minimize speaker similarity while maintaining usability constraints. The primary loss term is speaker embedding cosine similarity computed using ECAPA-TDNN. We add a regularization term ($\lambda_{\text{REG}} = 0.01$) that penalizes mask roughness and excessive noise strength to promote smooth, stable solutions.

\textbf{Constraint Handling}: We enforce SNR constraints in $[10, 20]$ dB through bounded reparameterization. The noise strength parameter $\gamma$ is internally represented in unbounded space and mapped to the allowed SNR range via sigmoid projection. This ensures the SNR constraint is satisfied at every optimization step without requiring explicit clipping.

\textbf{Optimization Algorithm}: We use the Adam optimizer~\citep{adam} with learning rate $\text{lr} = 0.01$ and default momentum parameters ($\beta_1 = 0.9$, $\beta_2 = 0.999$). We run optimization for 50 epochs per sample, which empirically provides good convergence. To ensure training stability, we apply gradient clipping with maximum norm 1.0.

\textbf{Initialization}: The temporal mask logits are initialized from a standard normal distribution, corresponding to a uniform mask ($m(t) \approx 0.5$) after sigmoid activation. The noise strength is initialized to produce SNR near the middle of the allowed range (approximately 15 dB).

\textbf{Computational Cost}: Optimization takes approximately 10-15 seconds per sample on a single NVIDIA RTX A6000 GPU. The total defense generation time for 168 samples is under 45 minutes, making the method practical for real-world deployment.

\subsection{Evaluation Protocol}

\textbf{Training Attack}: We simulate an attacker who fine-tunes a TTS model on protected recordings. To evaluate protection effectiveness, we measure speaker embedding quality and consistency on training data, then assess the similarity between embeddings extracted from clean and defended audio on held-out test samples. Lower test similarity indicates successful protection.

\textbf{Zero-Shot Attack}: We evaluate protection against zero-shot voice cloning, where an attacker uses protected recordings as reference audio for inference-time cloning. We generate synthetic speech using a pre-trained model with clean versus defended reference, measuring speaker similarity to the original speaker.

\textbf{Robustness Evaluation}: We test whether protection persists under five common audio preprocessing operations: MP3 compression (128 kbps and 64 kbps), spectral subtraction denoising, lowpass filtering (3400 Hz cutoff), and downsampling to 8 kHz. For each countermeasure, we re-compute speaker similarity and WER to assess protection retention.

\subsection{Evaluation Metrics}

\textbf{Speaker Similarity (SIM)}: We compute cosine similarity between speaker embeddings extracted from clean and defended (or synthesized) audio. Higher similarity indicates stronger speaker identity preservation. Protection effectiveness is measured as similarity degradation.

\textbf{Word Error Rate (WER)}: We measure the percentage of word-level errors (insertions, deletions, substitutions) between reference transcripts and ASR outputs. Lower WER indicates better intelligibility. We use WER to ensure protected speech remains usable.

\textbf{Perceptual Quality}: We employ PESQ~\citep{pesq} (Perceptual Evaluation of Speech Quality) and STOI~\citep{stoi} (Short-Time Objective Intelligibility) as objective quality metrics. PESQ ranges from -0.5 to 4.5 (higher is better), with scores above 3.0 considered good quality. STOI ranges from 0 to 1 (higher is better), with scores above 0.85 indicating high intelligibility.

\textbf{Mel-Cepstral Distortion (MCD)}: We compute MCD between clean and defended speech to quantify spectral distortion. Lower MCD indicates closer acoustic similarity.

\subsection{Baselines}

To demonstrate the advantage of scene-consistent noise, we compare SceneGuard against two baselines:

\textbf{Random Noise}: Uniform random noise sampled from $[-1, 1]$ and mixed at the same SNR range as SceneGuard. This baseline lacks scene consistency.

\textbf{Gaussian Noise}: Zero-mean Gaussian noise with unit variance, scaled to match SceneGuard's SNR range. This represents a simple additive noise baseline commonly used in audio processing.

\textbf{Clean (No Defense)}: Unprotected speech serves as an upper bound on cloning quality and a baseline for usability metrics.

\subsection{Statistical Analysis}

We employ rigorous statistical methods to validate our results:

\textbf{Bootstrap Confidence Intervals}: We compute 95\% confidence intervals for all metrics using bootstrap resampling with $n=10,000$ iterations. This provides robust uncertainty estimates without parametric assumptions.

\textbf{Hypothesis Testing}: We use permutation tests with $n=10,000$ iterations to assess statistical significance of protection effects. We report p-values and Cohen's $d$ effect sizes for key comparisons.

\textbf{Reproducibility}: All experiments use fixed random seeds (seed=1337) to ensure reproducibility. We provide complete implementation code and experiment configurations.

