\section{Method}

\subsection{Problem Formulation}

\textbf{Threat Model.}
We consider a training-time attack scenario where an adversary aims to clone a target speaker's voice by fine-tuning a pre-trained TTS or VC model on unauthorized audio recordings. The attacker operates in a black-box setting, having no knowledge of the protection mechanism applied to the training data. Formally, given a dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ of protected speech samples $x'_i$ and corresponding text labels $y_i$, the attacker solves:
\begin{equation}
\min_{\theta} \mathcal{L}_{\text{TTS}}(\theta; \mathcal{D}) = \frac{1}{N} \sum_{i=1}^N \ell(f_\theta(x'_i, y_i), x_i)
\end{equation}
where $f_\theta$ is the TTS/VC model with parameters $\theta$, and $\ell$ measures reconstruction quality.

\textbf{Defense Goal.}
The defender's objective is to apply a transformation $\mathcal{T}$ to speech $x$ that produces protected audio $x' = \mathcal{T}(x, s)$ (where $s$ denotes acoustic scene context) satisfying two criteria: (1) \textit{Protection}: degraded speaker identity prevents successful cloning, formally:
\begin{equation}
\text{SIM}(e(x'), e(x)) < \tau_{\text{sim}}, \quad \text{EER}(x', x) > \tau_{\text{eer}}
\end{equation}
where $e(\cdot)$ is a speaker verification encoder and $\tau_{\text{sim}}, \tau_{\text{eer}}$ are protection thresholds; and (2) \textit{Usability}: preserved intelligibility for legitimate communication:
\begin{equation}
\text{WER}(x') \leq \epsilon_{\text{wer}}, \quad \text{STOI}(x', x) \geq \tau_{\text{stoi}}
\end{equation}
where $\epsilon_{\text{wer}}$ is the maximum acceptable word error rate increase and $\tau_{\text{stoi}}$ is the minimum intelligibility threshold.

\textbf{Scene Consistency Requirement.}
Unlike random noise addition, we require that the protective transformation preserves acoustic scene consistency. Specifically, for a speech recording $x$ with scene label $s \in \mathcal{S}$ (e.g., ``park'', ``street\_traffic''), the protected audio $x'$ should remain plausibly associated with scene $s$:
\begin{equation}
P_{\text{ASC}}(s | x') \geq \tau_{\text{scene}}
\end{equation}
where $P_{\text{ASC}}$ is an acoustic scene classifier and $\tau_{\text{scene}}$ is a scene confidence threshold. This constraint ensures natural-sounding protection that is less likely to be removed by preprocessing.


\subsection{Scene-Consistent Audible Noise Defense}

\textbf{Limitations of Imperceptible Perturbations.}
Recent voice protection methods~\citep{safespeech,voiceblock} embed imperceptible adversarial perturbations by minimizing $\ell_p$ norms (typically $\|\delta\|_\infty \leq \epsilon$ with $\epsilon \approx 8/255$). While these perturbations are inaudible, they are \textit{fragile}: standard audio processing operations such as lossy compression (MP3, AAC), speech enhancement, or denoising can significantly attenuate protection effectiveness. For instance, SafeSpeech~\citep{safespeech} shows that DeepMucs~\citep{demucs} denoising reduces protection by up to 42\% (SIM increases from 0.204 to 0.284). This fragility arises because imperceptible perturbations occupy high-frequency or low-energy regions that are specifically targeted by perceptual codecs and noise reduction algorithms.

\textbf{Audible but Natural: Robustness Through Scene Consistency.}
We propose a fundamentally different paradigm: instead of imperceptible perturbations, we embed \textit{audible but scene-consistent background noise}. The key insight is that acoustic scene noise (e.g., traffic sounds on a street recording, airport terminal ambience) is perceptually expected and semantically legitimate. Speech enhancement algorithms are designed to \textit{preserve} such contextually appropriate noise, as aggressively removing it would degrade speech naturalness and introduce artifacts. Furthermore, audio codecs allocate more bits to perceptually important components, including scene-consistent noise.

Table~\ref{tab:paradigm_comparison} contrasts the three paradigms. Imperceptible perturbations achieve high naturalness but low robustness. Random audible noise is robust but unnatural (high perceptibility). Our scene-consistent approach balances both: moderate perceptibility but high robustness, as the noise is acoustically justified.

\begin{table}[t]
\centering
\caption{Comparison of voice protection paradigms}
\label{tab:paradigm_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Perceptibility} & \textbf{Naturalness} & \textbf{Robustness} \\
\midrule
Imperceptible~\citep{safespeech} & Low & High & Vulnerable \\
Random Audible & High & Low & Moderate \\
Scene-Consistent (Ours) & Moderate & High & High \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Need for Optimization.}
Naive mixing of speech with scene noise at a fixed signal-to-noise ratio (SNR) and uniform temporal weighting faces two risks: (1) insufficient protection if SNR is too high or noise placement is suboptimal, resulting in speaker embeddings that remain similar ($\text{SIM} > \tau_{\text{sim}}$); (2) excessive degradation if SNR is too low or noise overwhelms speech regions, causing unacceptable WER increases. To navigate this trade-off, we formulate protection as an \textit{optimization problem}: jointly optimize the temporal mask $m(t) \in [0,1]$ controlling when noise is applied, and the noise strength $\gamma$ controlling overall SNR, to minimize speaker similarity while maintaining usability. This optimization ensures that the noise is strategically placed (e.g., emphasizing pauses or non-critical phonemes) and calibrated to the specific speech sample.

Table~\ref{tab:method_comparison} provides a detailed comparison between SceneGuard and SafeSpeech~\citep{safespeech}, highlighting key methodological differences. While both methods employ gradient-based optimization, SceneGuard uses audible scene-consistent noise rather than imperceptible perturbations, trading some naturalness for significantly enhanced robustness.

\begin{table}[t]
\centering
\caption{Detailed method comparison: SafeSpeech vs. SceneGuard}
\label{tab:method_comparison}
\small
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{SafeSpeech} & \textbf{SceneGuard (Ours)} \\
\midrule
Noise Type & Imperceptible & Scene-Consistent Audible \\
Constraint & $\|\delta\|_\infty \leq 8/255$ & SNR $\in [10, 20]$ dB \\
Optimization Target & TTS error minimization & Speaker similarity minimization \\
Primary Loss & $\mathcal{L}_{\text{TTS}}$ (mel-spec) & $\mathcal{L}_{\text{SIM}}$ (embedding) \\
Temporal Control & Uniform $\delta(t)$ & Optimized mask $m(t)$ \\
Robustness (vs denoising) & Vulnerable ($\Delta$SIM $+0.08$) & Enhanced ($\Delta$SIM $-0.19$) \\
Optimization Time & $\sim$10.6s/sample & $\sim$10-15s/sample \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Optimization Objective}

\textbf{Mixing Model.}
Given clean speech $x(t)$ and a scene-consistent noise signal $n_k(t)$ sampled from a scene-specific library $\mathcal{N}_s$, we define the protected speech as:
\begin{equation}
\label{eq:mixing_model}
x'(t) = x(t) + \gamma \cdot m(t) \odot n_k(t)
\end{equation}
where $\gamma \in \mathbb{R}^+$ is a scalar noise strength, $m(t) \in [0,1]^T$ is a time-varying mask, and $\odot$ denotes element-wise multiplication. The mask $m(t)$ allows fine-grained temporal control: values near 1 apply full noise, values near 0 suppress noise. The noise $n_k$ is matched in length to $x$ via looping or truncation.

\textbf{Objective Function.}
We jointly optimize $m$ and $\gamma$ to minimize speaker similarity while regularizing for usability and smoothness. The total loss is:
\begin{equation}
\label{eq:total_loss}
\begin{aligned}
\mathcal{L}(m, \gamma) = \; & \lambda_{\text{SIM}} \cdot \mathcal{L}_{\text{SIM}}(m, \gamma) \\
& + \lambda_{\text{REG}} \cdot \mathcal{L}_{\text{REG}}(m, \gamma) \\
& + \lambda_{\text{ASR}} \cdot \mathcal{L}_{\text{ASR}}(m, \gamma) \\
& + \lambda_{\text{SCN}} \cdot \mathcal{L}_{\text{SCN}}(m, \gamma)
\end{aligned}
\end{equation}
subject to the SNR constraint:
\begin{equation}
\label{eq:snr_constraint}
\text{SNR}(x', n) \in [10, 20] \text{ dB}
\end{equation}
where SNR is computed as $10 \log_{10} \left( \frac{P_x}{P_{n'}} \right)$ with $P_x = \mathbb{E}[x(t)^2]$ and $P_{n'} = \mathbb{E}[(\gamma m(t) n_k(t))^2]$.

\textbf{Loss Components.}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Speaker Similarity Loss} $\mathcal{L}_{\text{SIM}}$: Minimizes cosine similarity between speaker embeddings of protected and clean audio:
    \begin{equation}
    \mathcal{L}_{\text{SIM}} = \text{sim}(e(x'), e(x)) = \frac{e(x') \cdot e(x)}{\|e(x')\| \|e(x)\|}
    \end{equation}
    where $e(\cdot)$ is a pre-trained speaker verification encoder (ECAPA-TDNN~\citep{ecapa}). This is the primary protection objective.
    
    \item \textbf{Regularization Loss} $\mathcal{L}_{\text{REG}}$: Encourages smooth masks and bounded noise strength to prevent extreme solutions:
    \begin{equation}
    \mathcal{L}_{\text{REG}} = \underbrace{\| \nabla m \|_2^2}_{\text{smoothness}} + \underbrace{\gamma^2}_{\text{energy penalty}}
    \end{equation}
    where $\nabla m = m(t+1) - m(t)$ are finite differences. Smoothness prevents spiky masks; energy penalty prevents excessively loud noise.
    
    \item \textbf{ASR Loss} $\mathcal{L}_{\text{ASR}}$ (optional): Penalizes transcription errors to maintain intelligibility. In practice, this is disabled ($\lambda_{\text{ASR}} = 0$) due to computational cost; usability is enforced via the SNR constraint instead.
    
    \item \textbf{Scene Consistency Loss} $\mathcal{L}_{\text{SCN}}$ (optional): Ensures protected audio retains scene label $s$ via negative log-likelihood from an acoustic scene classifier. Also disabled by default ($\lambda_{\text{SCN}} = 0$) as scene consistency is implicitly maintained by sampling noise from $\mathcal{N}_s$.
\end{itemize}

\textbf{Parameterization for Constrained Optimization.}
To enforce constraints via unconstrained optimization, we reparameterize:
\begin{equation}
\begin{aligned}
m(t) &= \sigma(\tilde{m}(t)), \quad \tilde{m} \in \mathbb{R}^T \\
\gamma &= \gamma_{\min} + (\gamma_{\max} - \gamma_{\min}) \cdot \sigma(\tilde{\gamma}), \quad \tilde{\gamma} \in \mathbb{R}
\end{aligned}
\end{equation}
where $\sigma(\cdot)$ is the sigmoid function, and $\gamma_{\min}, \gamma_{\max}$ are computed from the SNR bounds [10, 20] dB via:
\begin{equation}
\gamma_{\min} = \sqrt{\frac{P_x}{P_n \cdot 10^{\text{SNR}_{\max}/10}}}, \quad \gamma_{\max} = \sqrt{\frac{P_x}{P_n \cdot 10^{\text{SNR}_{\min}/10}}}
\end{equation}
where $P_n = \mathbb{E}[n_k(t)^2]$. This ensures $\gamma$ automatically satisfies Eq.~\eqref{eq:snr_constraint}.


\subsection{Temporal Mask Optimization Algorithm}

Algorithm~\ref{alg:sceneguard} presents the complete optimization procedure. We initialize $\tilde{m}$ and $\tilde{\gamma}$ randomly and optimize them via Adam~\citep{adam} with gradient clipping for stability. At each iteration, we project parameters to their constrained forms ($m \in [0,1]^T$, $\gamma$ via SNR bounds), apply the mixing model (Eq.~\eqref{eq:mixing_model}), compute losses, and perform a gradient step. The optimization converges in 50-100 epochs (approximately 10-30 seconds per sample on RTX A6000).

\begin{algorithm}[t]
\caption{SceneGuard Optimization}
\label{alg:sceneguard}
\KwIn{Speech $x \in \mathbb{R}^T$, scene label $s$, noise library $\mathcal{N}_s$, encoder $e$}
\KwOut{Protected speech $x' \in \mathbb{R}^T$}

\tcp{Initialize parameters}
Sample noise $n_k \sim \mathcal{N}_s$, match length to $x$\;
$\tilde{m} \leftarrow \mathcal{N}(0, 0.1^2)$ \tcp*{Random init near 0.5 after sigmoid}
$\tilde{\gamma} \leftarrow 0$ \tcp*{Midpoint of SNR range}
Compute $P_x = \mathbb{E}[x^2]$, $P_n = \mathbb{E}[n_k^2]$\;
$\text{optimizer} \leftarrow \text{Adam}([\tilde{m}, \tilde{\gamma}], \text{lr}=0.01)$\;

\tcp{Extract clean speaker embedding}
$e_x \leftarrow e(x)$ \tcp*{Target for similarity minimization}

\For{$\text{epoch} = 1, 2, \ldots, N_{\max}$}{
    \tcp{Project parameters to constrained domains}
    $m \leftarrow \sigma(\tilde{m})$ \tcp*{$m \in [0,1]^T$}
    $\gamma \leftarrow \text{project\_to\_snr}(\tilde{\gamma}, P_x, P_n)$ \tcp*{SNR $\in$ [10, 20] dB}
    
    \tcp{Forward: Mix speech with noise}
    $x' \leftarrow x + \gamma \cdot (m \odot n_k)$\;
    Normalize $x'$ to prevent clipping: $x' \leftarrow 0.99 \cdot x' / \max(|x'|)$\;
    
    \tcp{Compute losses}
    $e_{x'} \leftarrow e(x')$ \tcp*{Protected embedding}
    $\mathcal{L}_{\text{SIM}} \leftarrow \text{cosine\_sim}(e_{x'}, e_x)$\;
    $\mathcal{L}_{\text{REG}} \leftarrow \| m_{t+1} - m_t \|_2^2 + \gamma^2$\;
    $\mathcal{L} \leftarrow \lambda_{\text{SIM}} \mathcal{L}_{\text{SIM}} + \lambda_{\text{REG}} \mathcal{L}_{\text{REG}}$\;
    
    \tcp{Backward and update}
    $\nabla_{\tilde{m}}, \nabla_{\tilde{\gamma}} \leftarrow \text{autograd}(\mathcal{L})$\;
    Clip gradients: $\|\nabla\|_2 \leq 1.0$\;
    $\text{optimizer.step}()$\;
}

\Return $x' = x + \gamma \cdot (m \odot n_k)$\;
\end{algorithm}

\textbf{Training Stability.}
We employ three techniques for stable optimization: (1) \textit{Gradient clipping} with $\ell_2$ norm $\leq 1.0$ prevents exploding gradients, especially in early epochs when $\mathcal{L}_{\text{SIM}}$ changes rapidly. (2) \textit{Smoothness regularization} via the total variation term $\| \nabla m \|_2^2$ discourages abrupt mask transitions that could create audible artifacts. (3) \textit{Energy regularization} $\gamma^2$ prevents the optimizer from converging to excessively large $\gamma$ values that would violate the SNR constraint.


\subsection{Implementation Details}

\textbf{Noise Library Construction.}
We construct a scene-specific noise library from the TAU Urban Acoustic Scenes 2022 dataset~\citep{tau_dataset}, which contains 10 scene categories: \textit{airport}, \textit{bus}, \textit{metro}, \textit{metro\_station}, \textit{park}, \textit{public\_square}, \textit{shopping\_mall}, \textit{street\_pedestrian}, \textit{street\_traffic}, and \textit{tram}. For each scene $s \in \mathcal{S}$, we extract non-overlapping 3-second segments, resample to 16 kHz, RMS-normalize to unit energy, and filter out segments containing foreground speech (using Whisper~\citep{whisper} to verify WER $>$ 90\%). This yields a library of 50,000 noise clips (5,000 per scene) stored as $\mathcal{N}_s$.

\textbf{Scene Classification.}
To assign scene labels to speech recordings, we employ a deterministic hash-based assignment for reproducibility. Each audio file is assigned a scene label $s$ based on a hash of its file path: $s = \mathcal{S}[\text{hash}(\text{path}) \mod |\mathcal{S}|]$. While this approach does not leverage acoustic content, it ensures consistent scene-noise pairing across runs and enables controlled experimentation. Our defense framework is designed to be model-agnostic and compatible with any acoustic scene classifier; future work could integrate learned ASC models (e.g., PANNs~\citep{kong2020panns}) to improve scene accuracy in production settings.

\textbf{Hyperparameters.}
We set $\lambda_{\text{SIM}} = 1.0$ and $\lambda_{\text{REG}} = 0.01$ based on preliminary experiments on a validation set. The SNR range $[10, 20]$ dB is chosen to balance protection and usability: lower SNRs provide stronger protection but risk intelligibility loss, while higher SNRs preserve quality but may not sufficiently degrade speaker embeddings. We optimize for $N_{\max} = 50$ epochs using the Adam optimizer with learning rate 0.01. Optional losses $\mathcal{L}_{\text{ASR}}$ and $\mathcal{L}_{\text{SCN}}$ are disabled ($\lambda_{\text{ASR}} = \lambda_{\text{SCN}} = 0$) due to computational cost and marginal benefit over the SNR constraint.

\textbf{Computational Cost.}
On dual RTX A6000 GPUs, optimizing a single speech sample (average duration 4.5 seconds) requires approximately 10-15 seconds, dominated by speaker encoder forward passes (50 epochs $\times$ 2 passes/epoch). This is comparable to SafeSpeech~\citep{safespeech}, which reports 10.6 seconds per sample. For the full dataset of 100 training samples, total optimization time is approximately 20-30 minutes. Optimization is embarrassingly parallel across samples, enabling linear speedup with additional GPUs.

\textbf{Baseline Methods.}
For comparison, we implement two baseline mixing strategies: (1) \textit{Direct mixing} with random SNR $\sim \text{Uniform}(10, 20)$ dB and stochastic mask $m(t) \sim \text{Uniform}(0.6, 1.0)$, and (2) \textit{Random noise} using Gaussian noise instead of scene-consistent noise. These baselines isolate the contribution of optimization and scene consistency, respectively.
